{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc1c99f7-47e0-4fb4-bb23-e11c2c7918d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from github import Github\n",
    "import pickle\n",
    "import base64\n",
    "import uvicorn\n",
    "import joblib\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import psutil\n",
    "import datetime\n",
    "import os\n",
    "from enum import Enum\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee410883-34d1-4c75-9954-531914e569a5",
   "metadata": {},
   "source": [
    "# Execution Flow\n",
    "\n",
    "The application is designed to be run as a FastAPI server:\n",
    "\n",
    "## 1. Initialize the FastAPI App\n",
    "- Sets up middleware for **Cross-Origin Resource Sharing (CORS)**.\n",
    "- Adds routes for predictions and health checks.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Model Loading\n",
    "- Loads the trained ML model from GitHub upon service initialization.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Run Server\n",
    "- Uses `uvicorn` to run the FastAPI app on **port 8000** by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6606274-9cbc-4256-9e22-48a8df3f0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\anaconda3\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:149: UserWarning: Field \"model_status\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Retail Sales Prediction API\",\n",
    "    description=\"API for predicting retail sales using ML model\",\n",
    "    version=\"1.0\"\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Add CORS middleware\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "class HealthStatus(str, Enum):\n",
    "    HEALTHY = \"healthy\"\n",
    "    UNHEALTHY = \"unhealthy\"\n",
    "    DEGRADED = \"degraded\"\n",
    "\n",
    "class SystemHealth(BaseModel):\n",
    "    status: HealthStatus\n",
    "    timestamp: str\n",
    "    model_status: dict\n",
    "    system_info: dict\n",
    "    memory_usage: dict\n",
    "    disk_usage: dict\n",
    "    uptime: str\n",
    "\n",
    "class PredictionInput(BaseModel):\n",
    "    num_purchases: float\n",
    "    total_quantity: float\n",
    "    avg_quantity: float\n",
    "    std_quantity: float\n",
    "    avg_transaction: float\n",
    "    std_transaction: float\n",
    "    avg_unit_price: float\n",
    "    std_unit_price: float\n",
    "    country: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45245117-fe58-49e6-8506-241d18640924",
   "metadata": {},
   "source": [
    "# 1. Class: ModelService\n",
    "\n",
    "This class encapsulates all logic for managing the machine learning model, making predictions, and monitoring the system's health.\n",
    "\n",
    "## Purpose\n",
    "To load the model, handle predictions, and monitor metrics like prediction count, success rate, and resource usage.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `__init__()`\n",
    "Initializes the service:\n",
    "- Loads the model from a GitHub repository.\n",
    "- Defines the expected feature columns for prediction.\n",
    "- Tracks system uptime, prediction count, and failure count.\n",
    "\n",
    "#### Attributes:\n",
    "- **`model`**: Holds the machine learning model loaded from GitHub.\n",
    "- **`feature_columns`**: Expected input features for predictions.\n",
    "- **`start_time`**: Timestamp when the service started (for uptime tracking).\n",
    "- **`model_loaded`**: Boolean indicating whether the model was loaded successfully.\n",
    "- **`last_prediction_time`**: Tracks the timestamp of the last successful prediction.\n",
    "- **`total_predictions`**: Total number of predictions made.\n",
    "- **`failed_predictions`**: Total number of prediction failures.\n",
    "\n",
    "---\n",
    "\n",
    "### `load_model_from_github()`\n",
    "Downloads and loads the machine learning model (a `.pkl` file) from a GitHub repository.\n",
    "\n",
    "#### Steps:\n",
    "1. Sends a GET request to the raw URL of the model file on GitHub.\n",
    "2. Raises an error if the request fails.\n",
    "3. Deserializes the model using `pickle`.\n",
    "\n",
    "#### Error Handling:\n",
    "- Catches exceptions and sets `model_loaded` to `False` if loading fails.\n",
    "\n",
    "---\n",
    "\n",
    "### `predict(input_data: PredictionInput)`\n",
    "- Takes input data, processes it into a DataFrame, and makes a prediction using the loaded model.\n",
    "\n",
    "#### Updates Prediction Metrics:\n",
    "- **`last_prediction_time`**\n",
    "- **`total_predictions`**\n",
    "- **`failed_predictions`** (if an exception occurs)\n",
    "\n",
    "#### Error Handling:\n",
    "- If the prediction fails, it increments `failed_predictions`.\n",
    "\n",
    "---\n",
    "\n",
    "### `get_health_status()`\n",
    "Returns a detailed health status of the system and model, including:\n",
    "\n",
    "#### Memory Usage:\n",
    "- Total, available, and percentage memory usage (via `psutil`).\n",
    "\n",
    "#### Disk Usage:\n",
    "- Total, free, and percentage disk usage.\n",
    "\n",
    "#### Model Status:\n",
    "- Tracks if the model is loaded, the total predictions, failed predictions, and success rate.\n",
    "\n",
    "#### System Info:\n",
    "- CPU usage, core count, Python version, and uptime.\n",
    "\n",
    "#### Overall Status:\n",
    "- Marks the system as **HEALTHY**, **DEGRADED**, or **UNHEALTHY** based on resource usage and model availability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3347b01f-14c2-45b2-bcc0-e1bd96a7b8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "class ModelService:\n",
    "    def __init__(self):\n",
    "        self.model = self.load_model_from_github()\n",
    "        self.feature_columns = [\n",
    "            'num_purchases', 'total_quantity', 'avg_quantity', \n",
    "            'std_quantity', 'avg_transaction', 'std_transaction', \n",
    "            'avg_unit_price', 'std_unit_price', 'country'\n",
    "        ]\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.model_loaded = False\n",
    "        self.last_prediction_time = None\n",
    "        self.total_predictions = 0\n",
    "        self.failed_predictions = 0\n",
    "\n",
    "    def load_model_from_github(self):\n",
    "        \"\"\"Load model from GitHub repository\"\"\"\n",
    "        try:\n",
    "            # GitHub repository details\n",
    "            repo_owner = \"Group8MLUL2\"\n",
    "            repo_name = \"Group8_CT1\"\n",
    "            model_path = \"models/best_model.pkl\"\n",
    "            \n",
    "            # Step 1: Provide the raw URL of the .pkl file on GitHub\n",
    "            url = \"https://raw.githubusercontent.com/Group8MLUL2/Group8_CT1/main/models/best_model.pkl\"\n",
    "            \n",
    "            # Step 2: Download the file\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Check if the request was successful\n",
    "            \n",
    "            # Step 3: Load the pickle file into a Python object\n",
    "            model = pickle.loads(response.content)\n",
    "            self.model_loaded = True\n",
    "            print(\"Model loaded successfully!\")\n",
    "            return model\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {str(e)}\")\n",
    "            self.model_loaded = False\n",
    "            raise\n",
    "\n",
    "    def predict(self, input_data: PredictionInput) -> float:\n",
    "        \"\"\"Make prediction using the loaded model\"\"\"\n",
    "        try:\n",
    "            # Convert input to DataFrame\n",
    "            input_dict = input_data.dict()\n",
    "            df = pd.DataFrame([input_dict])\n",
    "            \n",
    "            # Make prediction\n",
    "            prediction = self.model.predict(df)\n",
    "            \n",
    "            # Update metrics\n",
    "            self.last_prediction_time = datetime.datetime.now()\n",
    "            self.total_predictions += 1\n",
    "            \n",
    "            return float(prediction[0])\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.failed_predictions += 1\n",
    "            print(f\"Error making prediction: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_health_status(self) -> SystemHealth:\n",
    "        \"\"\"Get comprehensive system health status\"\"\"\n",
    "        current_time = datetime.datetime.now()\n",
    "        \n",
    "        # Calculate uptime\n",
    "        uptime = current_time - self.start_time\n",
    "        \n",
    "        # Get memory usage\n",
    "        memory = psutil.virtual_memory()\n",
    "        memory_usage = {\n",
    "            \"total\": f\"{memory.total / (1024 * 1024 * 1024):.2f} GB\",\n",
    "            \"available\": f\"{memory.available / (1024 * 1024 * 1024):.2f} GB\",\n",
    "            \"percent\": f\"{memory.percent}%\",\n",
    "        }\n",
    "        \n",
    "        # Get disk usage\n",
    "        disk = psutil.disk_usage('/')\n",
    "        disk_usage = {\n",
    "            \"total\": f\"{disk.total / (1024 * 1024 * 1024):.2f} GB\",\n",
    "            \"free\": f\"{disk.free / (1024 * 1024 * 1024):.2f} GB\",\n",
    "            \"percent\": f\"{disk.percent}%\",\n",
    "        }\n",
    "        \n",
    "        # Get model status\n",
    "        model_status = {\n",
    "            \"loaded\": self.model_loaded,\n",
    "            \"total_predictions\": self.total_predictions,\n",
    "            \"failed_predictions\": self.failed_predictions,\n",
    "            \"last_prediction\": str(self.last_prediction_time) if self.last_prediction_time else \"Never\",\n",
    "            \"success_rate\": f\"{((self.total_predictions - self.failed_predictions) / self.total_predictions * 100):.2f}%\" if self.total_predictions > 0 else \"N/A\"\n",
    "        }\n",
    "        \n",
    "        # Get system info\n",
    "        system_info = {\n",
    "            \"cpu_percent\": f\"{psutil.cpu_percent()}%\",\n",
    "            \"cpu_count\": psutil.cpu_count(),\n",
    "            \"python_version\": os.sys.version,\n",
    "        }\n",
    "        \n",
    "        # Determine overall status\n",
    "        status = HealthStatus.HEALTHY\n",
    "        if not self.model_loaded:\n",
    "            status = HealthStatus.UNHEALTHY\n",
    "        elif memory.percent > 90 or disk.percent > 90:\n",
    "            status = HealthStatus.DEGRADED\n",
    "        \n",
    "        return SystemHealth(\n",
    "            status=status,\n",
    "            timestamp=str(current_time),\n",
    "            model_status=model_status,\n",
    "            system_info=system_info,\n",
    "            memory_usage=memory_usage,\n",
    "            disk_usage=disk_usage,\n",
    "            uptime=str(uptime)\n",
    "        )\n",
    "\n",
    "# # Initialize FastAPI app\n",
    "# app = FastAPI(\n",
    "#     title=\"Retail Sales Prediction API\",\n",
    "#     description=\"API for predicting retail sales using ML model\",\n",
    "#     version=\"1.0\"\n",
    "# )\n",
    "\n",
    "# Initialize model service\n",
    "model_service = ModelService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf971b-d038-42f0-a73b-862966438de0",
   "metadata": {},
   "source": [
    "# FastAPI Endpoints\n",
    "\n",
    "The application exposes several RESTful endpoints for health checks and predictions.\n",
    "\n",
    "## Endpoints\n",
    "\n",
    "### Root (`/`)\n",
    "- **Method**: `GET`\n",
    "- **Description**: Returns basic information about the API and available endpoints.\n",
    "\n",
    "---\n",
    "\n",
    "### Health Check (`/health`)\n",
    "- **Method**: `GET`\n",
    "- **Description**: Performs a basic health check.\n",
    "- **Response**:\n",
    "  - **`healthy`**: If the model is loaded successfully.\n",
    "  - **`unhealthy`**: Otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "### Detailed Health Check (`/health/full`)\n",
    "- **Method**: `GET`\n",
    "- **Description**: Returns a detailed health report, including:\n",
    "  - System resource usage.\n",
    "  - Uptime.\n",
    "  - Model status.\n",
    "- **Response Model**: Uses the `SystemHealth` class to structure the response.\n",
    "\n",
    "---\n",
    "\n",
    "### Prediction (`/predict`)\n",
    "- **Method**: `POST`\n",
    "- **Description**: Accepts input data and returns the model's prediction.\n",
    "- **Input**: JSON object matching the `PredictionInput` structure.\n",
    "- **Output**: JSON object containing:\n",
    "  - The prediction result.\n",
    "  - The input data.\n",
    "  - A timestamp for when the prediction was made.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4df5332-f724-4509-babf-8cf4ec0962ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    \"\"\"Root endpoint\"\"\"\n",
    "    return {\n",
    "        \"message\": \"Retail Sales Prediction API\",\n",
    "        \"status\": \"active\",\n",
    "        \"endpoints\": {\n",
    "            \"/predict\": \"Make predictions\",\n",
    "            \"/health\": \"Check API health\",\n",
    "            \"/health/full\": \"Detailed health check\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    \"\"\"Basic health check endpoint\"\"\"\n",
    "    return {\n",
    "        \"status\": \"healthy\" if model_service.model_loaded else \"unhealthy\",\n",
    "        \"timestamp\": datetime.datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "@app.get(\"/health/full\", response_model=SystemHealth)\n",
    "async def detailed_health():\n",
    "    \"\"\"Detailed health check endpoint\"\"\"\n",
    "    return model_service.get_health_status()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(input_data: PredictionInput):\n",
    "    \"\"\"Make prediction\"\"\"\n",
    "    try:\n",
    "        prediction = model_service.predict(input_data)\n",
    "        return {\n",
    "            \"prediction\": prediction,\n",
    "            \"input_data\": input_data.dict(),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f3997c1-780e-4e66-9422-183e393e616c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [30276]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:51752 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51756 - \"GET /health HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:51759 - \"POST /predict HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\divye\\AppData\\Local\\Temp\\ipykernel_30276\\1496101971.py:45: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  input_dict = input_data.dict()\n",
      "C:\\Users\\divye\\AppData\\Local\\Temp\\ipykernel_30276\\2657409222.py:34: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  \"input_data\": input_data.dict(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:51775 - \"GET /health HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [30276]\n"
     ]
    }
   ],
   "source": [
    "def run_server():\n",
    "    \"\"\"Run the FastAPI server with logging\"\"\"\n",
    "    try:\n",
    "        # logger.info(\"Starting API server\")\n",
    "        import uvicorn\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Server startup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_server()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
