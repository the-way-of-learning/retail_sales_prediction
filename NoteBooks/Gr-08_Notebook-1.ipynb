{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996843d-a1bb-4945-95d0-48401a68f819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn mlflow ydata-profiling openpyxl PyGithub xgboost psutil fastapi uvicorn streamlit plotly alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8922b6-40bd-4098-a47d-a0c88b1410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from ydata_profiling import ProfileReport\n",
    "from github import Github\n",
    "import base64\n",
    "import warnings\n",
    "import pickle\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b2d01-5f0e-4ff1-88fd-09646e69a0a6",
   "metadata": {},
   "source": [
    "# 1. GitHubManager\n",
    "\n",
    "This class is responsible for managing the interaction between the script and a GitHub repository. It handles uploading and updating files such as models, datasets, or reports to the repository.\n",
    "\n",
    "## Purpose\n",
    "To automate the process of saving data artifacts, reports, and machine learning models to GitHub for version control and collaboration.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `__init__(self, token=None, repo_name=\"Group8MLUL2/Group8_CT1\")`\n",
    "- Initializes the connection to the GitHub repository.\n",
    "- Takes a personal access token (`token`) and repository name (`repo_name`) as inputs.\n",
    "- Uses the GitHub API (via the `PyGithub` library) to authenticate and fetch the specified repository.\n",
    "\n",
    "### `create_or_update_file(self, path, content, message)`\n",
    "- Creates or updates a file in the repository.\n",
    "\n",
    "#### Steps:\n",
    "1. Serializes the content if it is a model (`Pipeline` object) or a pandas DataFrame (e.g., to pickle or Parquet format).\n",
    "2. Encodes the content in Base64 (required for GitHub's API).\n",
    "3. Checks if the file exists in the repository.  \n",
    "   - If it does: Updates the file with the new content.\n",
    "   - If it doesn’t: Creates a new file in the specified path.\n",
    "\n",
    "#### Handles:\n",
    "- ML models (as pickle files).\n",
    "- Processed data (e.g., Parquet format).\n",
    "- Text-based reports (e.g., HTML or plain text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a55b7d-5ae3-4fa9-8a75-280ea8da530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubManager:\n",
    "    \"\"\"\n",
    "    A class to handle interactions with a GitHub repository, such as uploading files\n",
    "    or updating models.\n",
    "    \"\"\"\n",
    "    def __init__(self, token=None, repo_name=\"Group8MLUL2/Group8_CT1\"):\n",
    "        \"\"\"\n",
    "        Initializes the GitHubManager with authentication and repository details.\n",
    "\n",
    "        Parameters:\n",
    "        - token (str): GitHub Personal Access Token for authentication.\n",
    "        - repo_name (str): Name of the repository to interact with (e.g., 'username/repo').\n",
    "        \"\"\"\n",
    "        self.github = Github(token) if token else Github()\n",
    "        self.repo = self.github.get_repo(repo_name)\n",
    "        print(f\"Connected to repository: {self.repo.html_url}\")\n",
    "\n",
    "    def create_or_update_file(self, path, content, message):\n",
    "        \"\"\"\n",
    "        Creates or updates a file in the GitHub repository.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): Path to the local file to be uploaded.\n",
    "        - content (str): Path in the repository where the file will be stored.\n",
    "        - message (str): Commit message to include with the file update.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle different content types\n",
    "            if isinstance(content, Pipeline) or hasattr(content, 'predict'):\n",
    "                # Serialize model using pickle\n",
    "                content_bytes = pickle.dumps(content)\n",
    "            elif isinstance(content, pd.DataFrame):\n",
    "                # Convert DataFrame to parquet bytes directly\n",
    "                buffer = io.BytesIO()\n",
    "                content.to_parquet(buffer)\n",
    "                content_bytes = buffer.getvalue()\n",
    "            elif isinstance(content, str):\n",
    "                content_bytes = content.encode()\n",
    "            else:\n",
    "                content_bytes = content\n",
    "\n",
    "            \n",
    "            try:\n",
    "                # Try to get the file contents to check if it exists\n",
    "                contents = self.repo.get_contents(path)\n",
    "                # File exists, update it\n",
    "                self.repo.update_file(path, message, content_bytes, contents.sha)\n",
    "                print(f\"Updated {path}\")\n",
    "            except Exception as e:\n",
    "                # File doesn't exist, create it\n",
    "                self.repo.create_file(path, message, content_base64)\n",
    "                print(f\"Created {path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bea2ec-b8cd-4cc4-a0ad-2115f310f2f9",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "\n",
    "To prepare the dataset for machine learning by cleaning, transforming, and generating meaningful features.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `load_data()`\n",
    "- Loads the raw dataset from a GitHub-hosted Excel file using `pd.read_excel()`.\n",
    "- Prints the shape of the dataset upon successful loading.\n",
    "\n",
    "---\n",
    "\n",
    "### `preprocess_data(df)`\n",
    "Cleans the raw dataset by:\n",
    "1. Removing rows with missing values (`dropna()`).\n",
    "2. Filtering out canceled orders (invoices starting with \"C\") and invalid data (e.g., `Quantity <= 0` or `UnitPrice <= 0`).\n",
    "3. Converting the `InvoiceDate` column to a datetime object.\n",
    "4. Extracting features like `Hour`, `Day`, `Month`, and `Year` from `InvoiceDate`.\n",
    "5. Calculating `TotalAmount` as `Quantity * UnitPrice`.\n",
    "\n",
    "**Output**: A cleaned and preprocessed dataset ready for feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### `create_features(df)`\n",
    "Aggregates customer-level features from the transactional data using `groupby()`:\n",
    "\n",
    "- Purchase frequency (`num_purchases`).\n",
    "- Quantity statistics (total, mean, std).\n",
    "- Transaction amount statistics (total, mean, std).\n",
    "- Average and standard deviation of unit prices.\n",
    "- The first occurrence of the customer's country.\n",
    "\n",
    "**Output**: A DataFrame of aggregated features for machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### `split_data(features, random_state=42)`\n",
    "Splits the feature dataset into three sets:\n",
    "- **Train**: 60% of the data for training models.\n",
    "- **Test**: 20% of the data for model evaluation.\n",
    "- **Production**: 20% of the data reserved for deployment scenarios.\n",
    "\n",
    "Uses `train_test_split()` from `sklearn` to ensure reproducibility with `random_state`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cecfea-4b55-4ab0-93d2-34470000bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "        \"\"\"Load data from the repository\"\"\"\n",
    "        url = \"https://raw.githubusercontent.com/Group8MLUL2/Group8_CT1/main/Online%20Retail.xlsx\"\n",
    "        df = pd.read_excel(url)\n",
    "        print(f\"Data loaded successfully. Shape: {df.info()}\")\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_data(df):\n",
    "        \"\"\"Preprocess the dataset\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        df = df.dropna()\n",
    "        print(f\"Shape after removing missing values: {df.shape}\")\n",
    "        \n",
    "        # Convert InvoiceDate to datetime\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        \n",
    "        # Extract features from datetime\n",
    "        df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "        df['Day'] = df['InvoiceDate'].dt.day\n",
    "        df['Month'] = df['InvoiceDate'].dt.month\n",
    "        df['Year'] = df['InvoiceDate'].dt.year\n",
    "        \n",
    "        # Calculate total amount\n",
    "        df['TotalAmount'] = df['Quantity'] * df['UnitPrice']\n",
    "        \n",
    "        # Filter out cancelled orders and invalid quantities/prices\n",
    "        df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "        df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_features(df):\n",
    "        \"\"\"Create features for ML\"\"\"\n",
    "        print(\"Creating features...\")\n",
    "        features = df.groupby('CustomerID').agg({\n",
    "            'InvoiceNo': 'count',\n",
    "            'Quantity': ['sum', 'mean', 'std'],\n",
    "            'TotalAmount': ['sum', 'mean', 'std'],\n",
    "            'UnitPrice': ['mean', 'std'],\n",
    "            'Country': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        features.columns = ['CustomerID', 'num_purchases', 'total_quantity', \n",
    "                          'avg_quantity', 'std_quantity', 'total_amount', \n",
    "                          'avg_transaction', 'std_transaction', 'avg_unit_price', \n",
    "                          'std_unit_price', 'country']\n",
    "        \n",
    "        print(f\"Features created. Shape: {features.shape}\")\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def split_data(features, random_state=42):\n",
    "        \"\"\"Split data into train, test, and production sets\"\"\"\n",
    "        train, remaining = train_test_split(features, train_size=0.6, random_state=random_state)\n",
    "        test, prod = train_test_split(remaining, test_size=0.5, random_state=random_state)\n",
    "        return train, test, prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216302e4-ac1e-4053-b2cc-bfa358c034b4",
   "metadata": {},
   "source": [
    "# 3. MLPipeline\n",
    "\n",
    "This class manages the machine learning pipeline, including preprocessing, model training, experiment tracking, and evaluation.\n",
    "\n",
    "## Purpose\n",
    "To streamline the process of training and evaluating multiple machine learning models, while logging metrics and artifacts in MLflow.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `__init__()`\n",
    "- Sets up MLflow for tracking experiments with a predefined `experiment_name`.\n",
    "- Points to an MLflow tracking server (default URI: `http://localhost:5000`).\n",
    "\n",
    "---\n",
    "\n",
    "### `create_preprocessing_pipeline()`\n",
    "Builds a `ColumnTransformer` pipeline to preprocess:\n",
    "\n",
    "#### Numerical Features:\n",
    "- Imputed with the median of the column.\n",
    "- Scaled using `StandardScaler()` for normalization.\n",
    "\n",
    "#### Categorical Features:\n",
    "- Imputed with a constant value (`'missing'`).\n",
    "- One-hot encoded (dropping the first category to avoid collinearity).\n",
    "\n",
    "**Returns**: A preprocessing pipeline ready to be plugged into a full ML pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### `run_experiment(model, model_name, X_train, y_train, X_test, y_test, params=None)`\n",
    "Runs a single machine learning experiment:\n",
    "1. Combines the preprocessing pipeline with a given model.\n",
    "2. Tracks parameters (`params`) and metrics (e.g., R², RMSE) in MLflow.\n",
    "3. Performs K-fold cross-validation to evaluate the model.\n",
    "4. Logs the trained model to MLflow (using appropriate format for `xgboost` or `sklearn`).\n",
    "\n",
    "---\n",
    "\n",
    "### `run_experiments(X_train, y_train, X_test, y_test)`\n",
    "Runs multiple experiments with different regression models:\n",
    "- **Linear Regression**\n",
    "- **Decision Tree Regressor**\n",
    "- **Random Forest Regressor**\n",
    "- **Support Vector Regressor**\n",
    "- **XGBoost Regressor**\n",
    "\n",
    "Tracks the performance of each model and identifies the best-performing model based on test R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc28fc-a030-4757-a367-d947b6f2c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline:\n",
    "    def __init__(self):\n",
    "        mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "        self.experiment_name = \"retail_sales_prediction\"\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"Create preprocessing pipeline for features\"\"\"\n",
    "        numeric_features = ['num_purchases', 'total_quantity', 'avg_quantity', \n",
    "                          'std_quantity', 'avg_transaction', 'std_transaction', \n",
    "                          'avg_unit_price', 'std_unit_price']\n",
    "        categorical_features = ['country']\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            # Set handle_unknown='ignore' to handle new categories\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        return preprocessor\n",
    "        \n",
    "    def run_experiment(self, model, model_name, X_train, y_train, X_test, y_test, params=None):\n",
    "        \"\"\"Run a single experiment with MLflow tracking\"\"\"\n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Create full pipeline\n",
    "        full_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "        \n",
    "        # Log parameters\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "        mlflow.log_params({\n",
    "            'model_type': model_name,\n",
    "            'training_samples': X_train.shape[0],\n",
    "            'features': X_train.shape[1]\n",
    "        })\n",
    "        \n",
    "        # K-fold cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(full_pipeline, X_train, y_train, \n",
    "                                  cv=kf, scoring='r2')\n",
    "        \n",
    "        # Train model\n",
    "        full_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = full_pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'cv_r2_mean': cv_scores.mean(),\n",
    "            'cv_r2_std': cv_scores.std(),\n",
    "            'test_mse': mse,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mae': mae\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        if isinstance(model, xgb.XGBRegressor):\n",
    "            mlflow.xgboost.log_model(model, model_name)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(full_pipeline, model_name)\n",
    "        \n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"Cross-validation R2: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "        print(f\"Test R2: {r2:.4f}\")\n",
    "        print(f\"Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"Test MAE: {mae:.4f}\")\n",
    "        \n",
    "        return full_pipeline, {\n",
    "            'cv_r2_mean': cv_scores.mean(),\n",
    "            'test_r2': r2,\n",
    "            'test_rmse': rmse\n",
    "        }\n",
    "\n",
    "    def run_experiments(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Run multiple experiments with different models and configurations\"\"\"\n",
    "        experiments = [\n",
    "            {\n",
    "                'name': 'LinearRegression',\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}\n",
    "            },\n",
    "            {\n",
    "                'name': 'DecisionTree',\n",
    "                'model': DecisionTreeRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'max_depth': 10,\n",
    "                    'min_samples_split': 5\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'RandomForest',\n",
    "                'model': RandomForestRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'max_depth': 15,\n",
    "                    'min_samples_split': 5\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'SVR',\n",
    "                'model': SVR(),\n",
    "                'params': {\n",
    "                    'kernel': 'rbf',\n",
    "                    'C': 1.0,\n",
    "                    'epsilon': 0.1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'XGBoost',\n",
    "                'model': xgb.XGBRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'max_depth': 6,\n",
    "                    'learning_rate': 0.1\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        best_model = None\n",
    "        best_score = -float('inf')\n",
    "        best_run_id = None\n",
    "        \n",
    "        for experiment in experiments:\n",
    "            print(f\"\\nRunning experiment: {experiment['name']}\")\n",
    "            with mlflow.start_run() as run:\n",
    "                model, metrics = self.run_experiment(\n",
    "                    experiment['model'],\n",
    "                    experiment['name'],\n",
    "                    X_train, y_train,\n",
    "                    X_test, y_test,\n",
    "                    experiment['params']\n",
    "                )\n",
    "                \n",
    "                results[experiment['name']] = metrics\n",
    "                run_id = run.info.run_id\n",
    "                \n",
    "                # Track best model\n",
    "                if metrics['test_r2'] > best_score:\n",
    "                    best_score = metrics['test_r2']\n",
    "                    best_model = model\n",
    "                    best_run_id = run_id\n",
    "\n",
    "        # Save best model in a separate run\n",
    "        with mlflow.start_run() as run:\n",
    "            best_model_name = max(results.items(), key=lambda x: x[1]['test_r2'])[0]\n",
    "            mlflow.log_param('best_model_type', best_model_name)\n",
    "            mlflow.log_param('best_run_id', best_run_id)\n",
    "            mlflow.log_metrics({\n",
    "                'best_model_r2': best_score\n",
    "            })\n",
    "            # Save the model directly\n",
    "            mlflow.sklearn.log_model(best_model, 'best_model')\n",
    "            model_uri = mlflow.get_artifact_uri('best_model')\n",
    "        \n",
    "        return results, best_model, model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fe23f-70a3-4097-bde8-e1f5ee70db23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3575c03-28d8-4a51-b697-2e43d4b45065",
   "metadata": {},
   "source": [
    "# Workflow Overview\n",
    "\n",
    "## 1. Initialize GitHubManager\n",
    "- Authenticate with GitHub using a personal access token and connect to the specified repository.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Processing\n",
    "- **Load the raw dataset.**\n",
    "- **Preprocess and clean the data.**\n",
    "- **Create customer-level features.**\n",
    "- **Split the data into train, test, and production sets.**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Generate Profile Report\n",
    "- Create a profile report for the dataset using `ydata_profiling`.\n",
    "- Upload the profile report to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd7208-2efb-40e6-b6a9-082cbf0fe973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " # Data Processing Pipeline\n",
    "print(\"\\n=== Data Processing Pipeline ===\")\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Load and process data\n",
    "df = processor.load_data()\n",
    "processed_df = processor.preprocess_data(df)\n",
    "features_df = processor.create_features(processed_df)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data, prod_data = processor.split_data(features_df)\n",
    "\n",
    "# Generate profile report\n",
    "print(\"\\nGenerating profile report...\")\n",
    "profile = ProfileReport(processed_df, title=\"Online Retail Dataset Profile\")\n",
    "\n",
    "# Save files back to the repository\n",
    "print(\"\\nSaving files to repository...\")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/train_data.parquet\",\n",
    "    train_data,\n",
    "    \"Update training data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/test_data.parquet\",\n",
    "    test_data,\n",
    "    \"Update test data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/prod_data.parquet\",\n",
    "    prod_data,\n",
    "    \"Update production data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"reports/profile_report.html\",\n",
    "    profile.to_html(),\n",
    "    \"Update profile report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f8ac1-eeba-473a-af08-3517ee873257",
   "metadata": {},
   "source": [
    "# Model Training and Saving Workflow\n",
    "\n",
    "## 1. Train Models\n",
    "- **Initialize the `MLPipeline`.**\n",
    "- **Train and evaluate multiple regression models.**\n",
    "- **Log metrics and artifacts to MLflow.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Save Best Model\n",
    "- **Identify the best model** based on test R².\n",
    "- Save the model in multiple formats (e.g., **Pickle**, **Joblib**).\n",
    "- **Upload the best model** to the GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fcc857-2f73-47fe-803d-c7dd79f6e3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mlflow.models.model\")\n",
    "    # Code that logs the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=my_model,  # Replace with your model\n",
    "        artifact_path=\"model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1950cd-7463-46ee-b976-eb3c6290c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Pipeline\n",
    "print(\"\\n=== ML Pipeline ===\")\n",
    "# Prepare features and target\n",
    "target_column = 'total_amount'\n",
    "feature_columns = [col for col in train_data.columns \n",
    "                 if col not in [target_column, 'CustomerID']]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Initialize and run ML pipeline\n",
    "ml_pipeline = MLPipeline()\n",
    "results, best_model, model_path = ml_pipeline.run_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b0a09-8f19-42eb-b199-01a77a6ec37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final results\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"CV R2: {metrics['cv_r2_mean']:.4f}\")\n",
    "    print(f\"Test R2: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "\n",
    "print(\"\\nSaving best model to repository...\")\n",
    "# Save the best model directly\n",
    "github_manager.create_or_update_file(\n",
    "    \"models/best_model.pkl\",\n",
    "    best_model,  # Pass the model object directly\n",
    "    \"Update best model\"\n",
    ")\n",
    "# Ensure models directory exists\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Method 1: Joblib save (recommended for sklearn models)\n",
    "joblib.dump(best_model, 'models/best_model_joblib.pkl', compress=True)\n",
    "\n",
    "# Method 2: Pickle save (alternative method)\n",
    "with open('models/best_model_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Also save model using joblib for backup\n",
    "# Method 1: Save to file\n",
    "joblib.dump(best_model, 'best_model_joblib.pkl')\n",
    "\n",
    "# Method 2: Save to a bytes buffer\n",
    "model_buffer = io.BytesIO()\n",
    "joblib.dump(best_model, model_buffer)\n",
    "model_bytes = model_buffer.getvalue()\n",
    "\n",
    "# When uploading to GitHub, use model_bytes\n",
    "github_manager.create_or_update_file(\n",
    "    \"models/best_model_joblib.pkl\",\n",
    "    model_bytes,\n",
    "    \"Update best model (joblib format)\"\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")\n",
    "print(f\"Repository URL: {github_manager.repo.html_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652367cd-6b5e-4c3d-96a1-02f1425ec197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
