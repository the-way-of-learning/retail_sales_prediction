{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8996843d-a1bb-4945-95d0-48401a68f819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in d:\\software\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy in d:\\software\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in d:\\software\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: mlflow in d:\\software\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: ydata-profiling in d:\\software\\anaconda3\\lib\\site-packages (4.12.0)\n",
      "Requirement already satisfied: openpyxl in d:\\software\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: PyGithub in d:\\software\\anaconda3\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: xgboost in d:\\software\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: psutil in d:\\software\\anaconda3\\lib\\site-packages (5.9.0)\n",
      "Requirement already satisfied: fastapi in d:\\software\\anaconda3\\lib\\site-packages (0.115.6)\n",
      "Requirement already satisfied: uvicorn in d:\\software\\anaconda3\\lib\\site-packages (0.33.0)\n",
      "Requirement already satisfied: streamlit in d:\\software\\anaconda3\\lib\\site-packages (1.32.0)\n",
      "Requirement already satisfied: plotly in d:\\software\\anaconda3\\lib\\site-packages (5.22.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\software\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\software\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\software\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in d:\\software\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: mlflow-skinny==2.19.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (2.19.0)\n",
      "Requirement already satisfied: Flask<4 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.0.3)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.1.4)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (1.13.3)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (7.1.0)\n",
      "Requirement already satisfied: graphene<4 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.4.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.8.4)\n",
      "Requirement already satisfied: pyarrow<19,>=4.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (14.0.2)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (2.0.30)\n",
      "Requirement already satisfied: waitress<4 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow) (3.0.2)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle<4 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (2.2.1)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (0.39.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (3.1.37)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (7.0.1)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (1.29.0)\n",
      "Requirement already satisfied: packaging<25 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (23.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (3.20.3)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (2.32.2)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.19.0->mlflow) (0.5.3)\n",
      "Requirement already satisfied: pydantic>=2 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.5.3)\n",
      "Requirement already satisfied: visions<0.7.7,>=0.7.5 in d:\\software\\anaconda3\\lib\\site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (0.7.6)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.4)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.66.4)\n",
      "Requirement already satisfied: seaborn<0.14,>=0.10.1 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.13.2)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.12)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.14.2)\n",
      "Requirement already satisfied: typeguard<5,>=3 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.4.1)\n",
      "Requirement already satisfied: imagehash==4.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.3 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.9.4)\n",
      "Requirement already satisfied: dacite>=1.8 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.8.1)\n",
      "Requirement already satisfied: numba<1,>=0.56.0 in d:\\software\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.59.1)\n",
      "Requirement already satisfied: PyWavelets in d:\\software\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (1.5.0)\n",
      "Requirement already satisfied: pillow in d:\\software\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (10.3.0)\n",
      "Requirement already satisfied: et-xmlfile in d:\\software\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from PyGithub) (1.5.0)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from PyGithub) (4.11.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in d:\\software\\anaconda3\\lib\\site-packages (from PyGithub) (2.2.2)\n",
      "Requirement already satisfied: Deprecated in d:\\software\\anaconda3\\lib\\site-packages (from PyGithub) (1.2.15)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in d:\\software\\anaconda3\\lib\\site-packages (from fastapi) (0.41.3)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\software\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (8.2.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (6.4.1)\n",
      "Requirement already satisfied: watchdog>=2.1.5 in d:\\software\\anaconda3\\lib\\site-packages (from streamlit) (4.0.1)\n",
      "Requirement already satisfied: Mako in d:\\software\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow) (1.2.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in d:\\software\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in d:\\software\\anaconda3\\lib\\site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: colorama in d:\\software\\anaconda3\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==2.19.0->mlflow) (0.4.6)\n",
      "Requirement already satisfied: pywin32>=304 in d:\\software\\anaconda3\\lib\\site-packages (from docker<8,>=4.0.0->mlflow) (305.1)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in d:\\software\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in d:\\software\\anaconda3\\lib\\site-packages (from Flask<4->mlflow) (2.2.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.7)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in d:\\software\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (3.2.5)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in d:\\software\\anaconda3\\lib\\site-packages (from graphene<4->mlflow) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow) (2.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\software\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\software\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\software\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow) (3.0.9)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in d:\\software\\anaconda3\\lib\\site-packages (from numba<1,>=0.56.0->ydata-profiling) (0.42.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in d:\\software\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (2.14.6)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (42.0.5)\n",
      "Requirement already satisfied: cffi>=1.4.1 in d:\\software\\anaconda3\\lib\\site-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
      "Requirement already satisfied: six>=1.5 in d:\\software\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.19.0->mlflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\software\\anaconda3\\lib\\site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in d:\\software\\anaconda3\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in d:\\software\\anaconda3\\lib\\site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (4.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in d:\\software\\anaconda3\\lib\\site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.6)\n",
      "Requirement already satisfied: attrs>=19.3.0 in d:\\software\\anaconda3\\lib\\site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (23.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in d:\\software\\anaconda3\\lib\\site-packages (from visions<0.7.7,>=0.7.5->visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (3.2.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in d:\\software\\anaconda3\\lib\\site-packages (from Deprecated->PyGithub) (1.14.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\software\\anaconda3\\lib\\site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.0)\n",
      "Requirement already satisfied: pycparser in d:\\software\\anaconda3\\lib\\site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
      "Requirement already satisfied: google-auth~=2.0 in d:\\software\\anaconda3\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (2.37.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in d:\\software\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.19.0->mlflow) (4.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\software\\anaconda3\\lib\\site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.19.0->mlflow) (3.17.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in d:\\software\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in d:\\software\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in d:\\software\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\software\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in d:\\software\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.19.0->mlflow) (0.50b0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\software\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\software\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\software\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.19.0->mlflow) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn mlflow ydata-profiling openpyxl PyGithub xgboost psutil fastapi uvicorn streamlit plotly alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e8922b6-40bd-4098-a47d-a0c88b1410d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.xgboost\n",
    "from ydata_profiling import ProfileReport\n",
    "from github import Github\n",
    "import base64\n",
    "import warnings\n",
    "import pickle\n",
    "import joblib\n",
    "warnings.filterwarnings('ignore')\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304b2d01-5f0e-4ff1-88fd-09646e69a0a6",
   "metadata": {},
   "source": [
    "# 1. GitHubManager\n",
    "\n",
    "This class is responsible for managing the interaction between the script and a GitHub repository. It handles uploading and updating files such as models, datasets, or reports to the repository.\n",
    "\n",
    "## Purpose\n",
    "To automate the process of saving data artifacts, reports, and machine learning models to GitHub for version control and collaboration.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `__init__(self, token=None, repo_name=\"Group8MLUL2/Group8_CT1\")`\n",
    "- Initializes the connection to the GitHub repository.\n",
    "- Takes a personal access token (`token`) and repository name (`repo_name`) as inputs.\n",
    "- Uses the GitHub API (via the `PyGithub` library) to authenticate and fetch the specified repository.\n",
    "\n",
    "### `create_or_update_file(self, path, content, message)`\n",
    "- Creates or updates a file in the repository.\n",
    "\n",
    "#### Steps:\n",
    "1. Serializes the content if it is a model (`Pipeline` object) or a pandas DataFrame (e.g., to pickle or Parquet format).\n",
    "2. Encodes the content in Base64 (required for GitHub's API).\n",
    "3. Checks if the file exists in the repository.  \n",
    "   - If it does: Updates the file with the new content.\n",
    "   - If it doesn’t: Creates a new file in the specified path.\n",
    "\n",
    "#### Handles:\n",
    "- ML models (as pickle files).\n",
    "- Processed data (e.g., Parquet format).\n",
    "- Text-based reports (e.g., HTML or plain text).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a55b7d-5ae3-4fa9-8a75-280ea8da530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GitHubManager:\n",
    "    \"\"\"\n",
    "    A class to handle interactions with a GitHub repository, such as uploading files\n",
    "    or updating models.\n",
    "    \"\"\"\n",
    "    def __init__(self, token=None, repo_name=\"Group8MLUL2/Group8_CT1\"):\n",
    "        \"\"\"\n",
    "        Initializes the GitHubManager with authentication and repository details.\n",
    "\n",
    "        Parameters:\n",
    "        - token (str): GitHub Personal Access Token for authentication.\n",
    "        - repo_name (str): Name of the repository to interact with (e.g., 'username/repo').\n",
    "        \"\"\"\n",
    "        self.github = Github(token) if token else Github()\n",
    "        self.repo = self.github.get_repo(repo_name)\n",
    "        print(f\"Connected to repository: {self.repo.html_url}\")\n",
    "\n",
    "    def create_or_update_file(self, path, content, message):\n",
    "        \"\"\"\n",
    "        Creates or updates a file in the GitHub repository.\n",
    "\n",
    "        Parameters:\n",
    "        - path (str): Path to the local file to be uploaded.\n",
    "        - content (str): Path in the repository where the file will be stored.\n",
    "        - message (str): Commit message to include with the file update.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle different content types\n",
    "            if isinstance(content, Pipeline) or hasattr(content, 'predict'):\n",
    "                # Serialize model using pickle\n",
    "                content_bytes = pickle.dumps(content)\n",
    "            elif isinstance(content, pd.DataFrame):\n",
    "                # Convert DataFrame to parquet bytes directly\n",
    "                buffer = io.BytesIO()\n",
    "                content.to_parquet(buffer)\n",
    "                content_bytes = buffer.getvalue()\n",
    "            elif isinstance(content, str):\n",
    "                content_bytes = content.encode()\n",
    "            else:\n",
    "                content_bytes = content\n",
    "\n",
    "            \n",
    "            try:\n",
    "                # Try to get the file contents to check if it exists\n",
    "                contents = self.repo.get_contents(path)\n",
    "                # File exists, update it\n",
    "                self.repo.update_file(path, message, content_bytes, contents.sha)\n",
    "                print(f\"Updated {path}\")\n",
    "            except Exception as e:\n",
    "                # File doesn't exist, create it\n",
    "                self.repo.create_file(path, message, content_base64)\n",
    "                print(f\"Created {path}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bea2ec-b8cd-4cc4-a0ad-2115f310f2f9",
   "metadata": {},
   "source": [
    "# Data Preparation for Machine Learning\n",
    "\n",
    "To prepare the dataset for machine learning by cleaning, transforming, and generating meaningful features.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `load_data()`\n",
    "- Loads the raw dataset from a GitHub-hosted Excel file using `pd.read_excel()`.\n",
    "- Prints the shape of the dataset upon successful loading.\n",
    "\n",
    "---\n",
    "\n",
    "### `preprocess_data(df)`\n",
    "Cleans the raw dataset by:\n",
    "1. Removing rows with missing values (`dropna()`).\n",
    "2. Filtering out canceled orders (invoices starting with \"C\") and invalid data (e.g., `Quantity <= 0` or `UnitPrice <= 0`).\n",
    "3. Converting the `InvoiceDate` column to a datetime object.\n",
    "4. Extracting features like `Hour`, `Day`, `Month`, and `Year` from `InvoiceDate`.\n",
    "5. Calculating `TotalAmount` as `Quantity * UnitPrice`.\n",
    "\n",
    "**Output**: A cleaned and preprocessed dataset ready for feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "### `create_features(df)`\n",
    "Aggregates customer-level features from the transactional data using `groupby()`:\n",
    "\n",
    "- Purchase frequency (`num_purchases`).\n",
    "- Quantity statistics (total, mean, std).\n",
    "- Transaction amount statistics (total, mean, std).\n",
    "- Average and standard deviation of unit prices.\n",
    "- The first occurrence of the customer's country.\n",
    "\n",
    "**Output**: A DataFrame of aggregated features for machine learning.\n",
    "\n",
    "---\n",
    "\n",
    "### `split_data(features, random_state=42)`\n",
    "Splits the feature dataset into three sets:\n",
    "- **Train**: 60% of the data for training models.\n",
    "- **Test**: 20% of the data for model evaluation.\n",
    "- **Production**: 20% of the data reserved for deployment scenarios.\n",
    "\n",
    "Uses `train_test_split()` from `sklearn` to ensure reproducibility with `random_state`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88cecfea-4b55-4ab0-93d2-34470000bb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    @staticmethod\n",
    "    def load_data():\n",
    "        \"\"\"Load data from the repository\"\"\"\n",
    "        url = \"https://raw.githubusercontent.com/Group8MLUL2/Group8_CT1/main/Online%20Retail.xlsx\"\n",
    "        df = pd.read_excel(url)\n",
    "        print(f\"Data loaded successfully. Shape: {df.info()}\")\n",
    "        print(df)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_data(df):\n",
    "        \"\"\"Preprocess the dataset\"\"\"\n",
    "        print(\"Preprocessing data...\")\n",
    "        print(f\"Initial shape: {df.shape}\")\n",
    "        \n",
    "        # Remove rows with missing values\n",
    "        df = df.dropna()\n",
    "        print(f\"Shape after removing missing values: {df.shape}\")\n",
    "        \n",
    "        # Convert InvoiceDate to datetime\n",
    "        df['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'])\n",
    "        \n",
    "        # Extract features from datetime\n",
    "        df['Hour'] = df['InvoiceDate'].dt.hour\n",
    "        df['Day'] = df['InvoiceDate'].dt.day\n",
    "        df['Month'] = df['InvoiceDate'].dt.month\n",
    "        df['Year'] = df['InvoiceDate'].dt.year\n",
    "        \n",
    "        # Calculate total amount\n",
    "        df['TotalAmount'] = df['Quantity'] * df['UnitPrice']\n",
    "        \n",
    "        # Filter out cancelled orders and invalid quantities/prices\n",
    "        df = df[~df['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "        df = df[(df['Quantity'] > 0) & (df['UnitPrice'] > 0)]\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_features(df):\n",
    "        \"\"\"Create features for ML\"\"\"\n",
    "        print(\"Creating features...\")\n",
    "        features = df.groupby('CustomerID').agg({\n",
    "            'InvoiceNo': 'count',\n",
    "            'Quantity': ['sum', 'mean', 'std'],\n",
    "            'TotalAmount': ['sum', 'mean', 'std'],\n",
    "            'UnitPrice': ['mean', 'std'],\n",
    "            'Country': 'first'\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Flatten column names\n",
    "        features.columns = ['CustomerID', 'num_purchases', 'total_quantity', \n",
    "                          'avg_quantity', 'std_quantity', 'total_amount', \n",
    "                          'avg_transaction', 'std_transaction', 'avg_unit_price', \n",
    "                          'std_unit_price', 'country']\n",
    "        \n",
    "        print(f\"Features created. Shape: {features.shape}\")\n",
    "        return features\n",
    "\n",
    "    @staticmethod\n",
    "    def split_data(features, random_state=42):\n",
    "        \"\"\"Split data into train, test, and production sets\"\"\"\n",
    "        train, remaining = train_test_split(features, train_size=0.6, random_state=random_state)\n",
    "        test, prod = train_test_split(remaining, test_size=0.5, random_state=random_state)\n",
    "        return train, test, prod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216302e4-ac1e-4053-b2cc-bfa358c034b4",
   "metadata": {},
   "source": [
    "# 3. MLPipeline\n",
    "\n",
    "This class manages the machine learning pipeline, including preprocessing, model training, experiment tracking, and evaluation.\n",
    "\n",
    "## Purpose\n",
    "To streamline the process of training and evaluating multiple machine learning models, while logging metrics and artifacts in MLflow.\n",
    "\n",
    "## Key Methods\n",
    "\n",
    "### `__init__()`\n",
    "- Sets up MLflow for tracking experiments with a predefined `experiment_name`.\n",
    "- Points to an MLflow tracking server (default URI: `http://localhost:5000`).\n",
    "\n",
    "---\n",
    "\n",
    "### `create_preprocessing_pipeline()`\n",
    "Builds a `ColumnTransformer` pipeline to preprocess:\n",
    "\n",
    "#### Numerical Features:\n",
    "- Imputed with the median of the column.\n",
    "- Scaled using `StandardScaler()` for normalization.\n",
    "\n",
    "#### Categorical Features:\n",
    "- Imputed with a constant value (`'missing'`).\n",
    "- One-hot encoded (dropping the first category to avoid collinearity).\n",
    "\n",
    "**Returns**: A preprocessing pipeline ready to be plugged into a full ML pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### `run_experiment(model, model_name, X_train, y_train, X_test, y_test, params=None)`\n",
    "Runs a single machine learning experiment:\n",
    "1. Combines the preprocessing pipeline with a given model.\n",
    "2. Tracks parameters (`params`) and metrics (e.g., R², RMSE) in MLflow.\n",
    "3. Performs K-fold cross-validation to evaluate the model.\n",
    "4. Logs the trained model to MLflow (using appropriate format for `xgboost` or `sklearn`).\n",
    "\n",
    "---\n",
    "\n",
    "### `run_experiments(X_train, y_train, X_test, y_test)`\n",
    "Runs multiple experiments with different regression models:\n",
    "- **Linear Regression**\n",
    "- **Decision Tree Regressor**\n",
    "- **Random Forest Regressor**\n",
    "- **Support Vector Regressor**\n",
    "- **XGBoost Regressor**\n",
    "\n",
    "Tracks the performance of each model and identifies the best-performing model based on test R².\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38bc28fc-a030-4757-a367-d947b6f2c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeline:\n",
    "    def __init__(self):\n",
    "        mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "        self.experiment_name = \"retail_sales_prediction\"\n",
    "        mlflow.set_experiment(self.experiment_name)\n",
    "        \n",
    "    def create_preprocessing_pipeline(self):\n",
    "        \"\"\"Create preprocessing pipeline for features\"\"\"\n",
    "        numeric_features = ['num_purchases', 'total_quantity', 'avg_quantity', \n",
    "                          'std_quantity', 'avg_transaction', 'std_transaction', \n",
    "                          'avg_unit_price', 'std_unit_price']\n",
    "        categorical_features = ['country']\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        \n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            # Set handle_unknown='ignore' to handle new categories\n",
    "            ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "        ])\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "        \n",
    "        return preprocessor\n",
    "        \n",
    "    def run_experiment(self, model, model_name, X_train, y_train, X_test, y_test, params=None):\n",
    "        \"\"\"Run a single experiment with MLflow tracking\"\"\"\n",
    "        # Create preprocessing pipeline\n",
    "        preprocessor = self.create_preprocessing_pipeline()\n",
    "        \n",
    "        # Create full pipeline\n",
    "        full_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('regressor', model)\n",
    "        ])\n",
    "        \n",
    "        # Log parameters\n",
    "        if params:\n",
    "            mlflow.log_params(params)\n",
    "        mlflow.log_params({\n",
    "            'model_type': model_name,\n",
    "            'training_samples': X_train.shape[0],\n",
    "            'features': X_train.shape[1]\n",
    "        })\n",
    "        \n",
    "        # K-fold cross-validation\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        cv_scores = cross_val_score(full_pipeline, X_train, y_train, \n",
    "                                  cv=kf, scoring='r2')\n",
    "        \n",
    "        # Train model\n",
    "        full_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = full_pipeline.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            'cv_r2_mean': cv_scores.mean(),\n",
    "            'cv_r2_std': cv_scores.std(),\n",
    "            'test_mse': mse,\n",
    "            'test_rmse': rmse,\n",
    "            'test_r2': r2,\n",
    "            'test_mae': mae\n",
    "        })\n",
    "        \n",
    "        # Log model\n",
    "        if isinstance(model, xgb.XGBRegressor):\n",
    "            mlflow.xgboost.log_model(model, model_name)\n",
    "        else:\n",
    "            mlflow.sklearn.log_model(full_pipeline, model_name)\n",
    "        \n",
    "        print(f\"\\nResults for {model_name}:\")\n",
    "        print(f\"Cross-validation R2: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "        print(f\"Test R2: {r2:.4f}\")\n",
    "        print(f\"Test RMSE: {rmse:.4f}\")\n",
    "        print(f\"Test MAE: {mae:.4f}\")\n",
    "        \n",
    "        return full_pipeline, {\n",
    "            'cv_r2_mean': cv_scores.mean(),\n",
    "            'test_r2': r2,\n",
    "            'test_rmse': rmse\n",
    "        }\n",
    "\n",
    "    def run_experiments(self, X_train, y_train, X_test, y_test):\n",
    "        \"\"\"Run multiple experiments with different models and configurations\"\"\"\n",
    "        experiments = [\n",
    "            {\n",
    "                'name': 'LinearRegression',\n",
    "                'model': LinearRegression(),\n",
    "                'params': {}\n",
    "            },\n",
    "            {\n",
    "                'name': 'DecisionTree',\n",
    "                'model': DecisionTreeRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'max_depth': 10,\n",
    "                    'min_samples_split': 5\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'RandomForest',\n",
    "                'model': RandomForestRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'max_depth': 15,\n",
    "                    'min_samples_split': 5\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'SVR',\n",
    "                'model': SVR(),\n",
    "                'params': {\n",
    "                    'kernel': 'rbf',\n",
    "                    'C': 1.0,\n",
    "                    'epsilon': 0.1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'XGBoost',\n",
    "                'model': xgb.XGBRegressor(random_state=42),\n",
    "                'params': {\n",
    "                    'n_estimators': 100,\n",
    "                    'max_depth': 6,\n",
    "                    'learning_rate': 0.1\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = {}\n",
    "        best_model = None\n",
    "        best_score = -float('inf')\n",
    "        best_run_id = None\n",
    "        \n",
    "        for experiment in experiments:\n",
    "            print(f\"\\nRunning experiment: {experiment['name']}\")\n",
    "            with mlflow.start_run() as run:\n",
    "                model, metrics = self.run_experiment(\n",
    "                    experiment['model'],\n",
    "                    experiment['name'],\n",
    "                    X_train, y_train,\n",
    "                    X_test, y_test,\n",
    "                    experiment['params']\n",
    "                )\n",
    "                \n",
    "                results[experiment['name']] = metrics\n",
    "                run_id = run.info.run_id\n",
    "                \n",
    "                # Track best model\n",
    "                if metrics['test_r2'] > best_score:\n",
    "                    best_score = metrics['test_r2']\n",
    "                    best_model = model\n",
    "                    best_run_id = run_id\n",
    "\n",
    "        # Save best model in a separate run\n",
    "        with mlflow.start_run() as run:\n",
    "            best_model_name = max(results.items(), key=lambda x: x[1]['test_r2'])[0]\n",
    "            mlflow.log_param('best_model_type', best_model_name)\n",
    "            mlflow.log_param('best_run_id', best_run_id)\n",
    "            mlflow.log_metrics({\n",
    "                'best_model_r2': best_score\n",
    "            })\n",
    "            # Save the model directly\n",
    "            mlflow.sklearn.log_model(best_model, 'best_model')\n",
    "            model_uri = mlflow.get_artifact_uri('best_model')\n",
    "        \n",
    "        return results, best_model, model_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "570fe23f-70a3-4097-bde8-e1f5ee70db23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to repository: https://github.com/Group8MLUL2/Group8_CT1\n"
     ]
    }
   ],
   "source": [
    "#Setup Git hub Connection\n",
    "# Initialize GitHub manager\n",
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3575c03-28d8-4a51-b697-2e43d4b45065",
   "metadata": {},
   "source": [
    "# Workflow Overview\n",
    "\n",
    "## 1. Initialize GitHubManager\n",
    "- Authenticate with GitHub using a personal access token and connect to the specified repository.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Data Processing\n",
    "- **Load the raw dataset.**\n",
    "- **Preprocess and clean the data.**\n",
    "- **Create customer-level features.**\n",
    "- **Split the data into train, test, and production sets.**\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Generate Profile Report\n",
    "- Create a profile report for the dataset using `ydata_profiling`.\n",
    "- Upload the profile report to GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61dd7208-2efb-40e6-b6a9-082cbf0fe973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Data Processing Pipeline ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541909 entries, 0 to 541908\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   InvoiceNo    541909 non-null  object        \n",
      " 1   StockCode    541909 non-null  object        \n",
      " 2   Description  540455 non-null  object        \n",
      " 3   Quantity     541909 non-null  int64         \n",
      " 4   InvoiceDate  541909 non-null  datetime64[ns]\n",
      " 5   UnitPrice    541909 non-null  float64       \n",
      " 6   CustomerID   406829 non-null  float64       \n",
      " 7   Country      541909 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 33.1+ MB\n",
      "Data loaded successfully. Shape: None\n",
      "       InvoiceNo StockCode                          Description  Quantity  \\\n",
      "0         536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
      "1         536365     71053                  WHITE METAL LANTERN         6   \n",
      "2         536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
      "3         536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
      "4         536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
      "...          ...       ...                                  ...       ...   \n",
      "541904    581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
      "541905    581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
      "541906    581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
      "541907    581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
      "541908    581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
      "\n",
      "               InvoiceDate  UnitPrice  CustomerID         Country  \n",
      "0      2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
      "1      2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "2      2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
      "3      2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "4      2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
      "...                    ...        ...         ...             ...  \n",
      "541904 2011-12-09 12:50:00       0.85     12680.0          France  \n",
      "541905 2011-12-09 12:50:00       2.10     12680.0          France  \n",
      "541906 2011-12-09 12:50:00       4.15     12680.0          France  \n",
      "541907 2011-12-09 12:50:00       4.15     12680.0          France  \n",
      "541908 2011-12-09 12:50:00       4.95     12680.0          France  \n",
      "\n",
      "[541909 rows x 8 columns]\n",
      "Preprocessing data...\n",
      "Initial shape: (541909, 8)\n",
      "Shape after removing missing values: (406829, 8)\n",
      "Final shape: (397884, 13)\n",
      "Creating features...\n",
      "Features created. Shape: (4338, 11)\n",
      "\n",
      "Generating profile report...\n",
      "\n",
      "Saving files to repository...\n",
      "Updated processed/train_data.parquet\n",
      "Updated processed/test_data.parquet\n",
      "Updated processed/prod_data.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2821b462979d45f3931a80358fa1f65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde7036fe7cc4356ac0398a043efce26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0377eae20e4c2589e78579a3a5ab18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated reports/profile_report.html\n"
     ]
    }
   ],
   "source": [
    " # Data Processing Pipeline\n",
    "print(\"\\n=== Data Processing Pipeline ===\")\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Load and process data\n",
    "df = processor.load_data()\n",
    "processed_df = processor.preprocess_data(df)\n",
    "features_df = processor.create_features(processed_df)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data, prod_data = processor.split_data(features_df)\n",
    "\n",
    "# Generate profile report\n",
    "print(\"\\nGenerating profile report...\")\n",
    "profile = ProfileReport(processed_df, title=\"Online Retail Dataset Profile\")\n",
    "\n",
    "# Save files back to the repository\n",
    "print(\"\\nSaving files to repository...\")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/train_data.parquet\",\n",
    "    train_data,\n",
    "    \"Update training data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/test_data.parquet\",\n",
    "    test_data,\n",
    "    \"Update test data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"processed/prod_data.parquet\",\n",
    "    prod_data,\n",
    "    \"Update production data\"\n",
    ")\n",
    "github_manager.create_or_update_file(\n",
    "    \"reports/profile_report.html\",\n",
    "    profile.to_html(),\n",
    "    \"Update profile report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f8ac1-eeba-473a-af08-3517ee873257",
   "metadata": {},
   "source": [
    "# Model Training and Saving Workflow\n",
    "\n",
    "## 1. Train Models\n",
    "- **Initialize the `MLPipeline`.**\n",
    "- **Train and evaluate multiple regression models.**\n",
    "- **Log metrics and artifacts to MLflow.**\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Save Best Model\n",
    "- **Identify the best model** based on test R².\n",
    "- Save the model in multiple formats (e.g., **Pickle**, **Joblib**).\n",
    "- **Upload the best model** to the GitHub repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0fcc857-2f73-47fe-803d-c7dd79f6e3c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mUserWarning\u001b[39;00m, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmlflow.models.model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Code that logs the model\u001b[39;00m\n\u001b[0;32m      6\u001b[0m mlflow\u001b[38;5;241m.\u001b[39msklearn\u001b[38;5;241m.\u001b[39mlog_model(\n\u001b[1;32m----> 7\u001b[0m     sk_model\u001b[38;5;241m=\u001b[39mmy_model,  \u001b[38;5;66;03m# Replace with your model\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'my_model' is not defined"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"mlflow.models.model\")\n",
    "    # Code that logs the model\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=my_model,  # Replace with your model\n",
    "        artifact_path=\"model\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f1950cd-7463-46ee-b976-eb3c6290c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ML Pipeline ===\n",
      "\n",
      "Running experiment: LinearRegression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:47:57 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for LinearRegression:\n",
      "Cross-validation R2: -2.4541 (+/- 13.5032)\n",
      "Test R2: 0.5380\n",
      "Test RMSE: 6673.3233\n",
      "Test MAE: 762.9633\n",
      "🏃 View run bittersweet-goat-924 at: http://localhost:5000/#/experiments/906212838224541278/runs/1decc1d32b5e451fabdd1561c13bd2d0\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n",
      "\n",
      "Running experiment: DecisionTree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:48:04 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for DecisionTree:\n",
      "Cross-validation R2: 0.6629 (+/- 0.3624)\n",
      "Test R2: 0.6146\n",
      "Test RMSE: 6094.4603\n",
      "Test MAE: 648.1361\n",
      "🏃 View run bustling-gnu-58 at: http://localhost:5000/#/experiments/906212838224541278/runs/f39561fbcfa2454baca1fea08824e017\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n",
      "\n",
      "Running experiment: RandomForest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:48:34 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for RandomForest:\n",
      "Cross-validation R2: 0.8497 (+/- 0.1382)\n",
      "Test R2: 0.6574\n",
      "Test RMSE: 5746.4677\n",
      "Test MAE: 513.5434\n",
      "🏃 View run adventurous-cow-298 at: http://localhost:5000/#/experiments/906212838224541278/runs/e82bc7a98e1840ec921978a08ef393a2\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n",
      "\n",
      "Running experiment: SVR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:48:47 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for SVR:\n",
      "Cross-validation R2: -0.0303 (+/- 0.0288)\n",
      "Test R2: -0.0156\n",
      "Test RMSE: 9893.9251\n",
      "Test MAE: 1672.3676\n",
      "🏃 View run legendary-cow-738 at: http://localhost:5000/#/experiments/906212838224541278/runs/092e193736a541d1aef7a956509fc755\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n",
      "\n",
      "Running experiment: XGBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:48:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results for XGBoost:\n",
      "Cross-validation R2: 0.7644 (+/- 0.3219)\n",
      "Test R2: 0.6310\n",
      "Test RMSE: 5963.2575\n",
      "Test MAE: 503.7949\n",
      "🏃 View run upset-lamb-13 at: http://localhost:5000/#/experiments/906212838224541278/runs/8a606508f0e4428cae796a27cc248ddb\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/15 16:49:07 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run adorable-dove-201 at: http://localhost:5000/#/experiments/906212838224541278/runs/9e16f380372f446da6434648657803fc\n",
      "🧪 View experiment at: http://localhost:5000/#/experiments/906212838224541278\n"
     ]
    }
   ],
   "source": [
    "# ML Pipeline\n",
    "print(\"\\n=== ML Pipeline ===\")\n",
    "# Prepare features and target\n",
    "target_column = 'total_amount'\n",
    "feature_columns = [col for col in train_data.columns \n",
    "                 if col not in [target_column, 'CustomerID']]\n",
    "\n",
    "X_train = train_data[feature_columns]\n",
    "y_train = train_data[target_column]\n",
    "X_test = test_data[feature_columns]\n",
    "y_test = test_data[target_column]\n",
    "\n",
    "# Initialize and run ML pipeline\n",
    "ml_pipeline = MLPipeline()\n",
    "results, best_model, model_path = ml_pipeline.run_experiments(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d3b0a09-8f19-42eb-b199-01a77a6ec37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Final Results ===\n",
      "\n",
      "LinearRegression:\n",
      "CV R2: -2.4541\n",
      "Test R2: 0.5380\n",
      "Test RMSE: 6673.3233\n",
      "\n",
      "DecisionTree:\n",
      "CV R2: 0.6629\n",
      "Test R2: 0.6146\n",
      "Test RMSE: 6094.4603\n",
      "\n",
      "RandomForest:\n",
      "CV R2: 0.8497\n",
      "Test R2: 0.6574\n",
      "Test RMSE: 5746.4677\n",
      "\n",
      "SVR:\n",
      "CV R2: -0.0303\n",
      "Test R2: -0.0156\n",
      "Test RMSE: 9893.9251\n",
      "\n",
      "XGBoost:\n",
      "CV R2: 0.7644\n",
      "Test R2: 0.6310\n",
      "Test RMSE: 5963.2575\n",
      "\n",
      "Saving best model to repository...\n",
      "Updated models/best_model.pkl\n",
      "Updated models/best_model_joblib.pkl\n",
      "\n",
      "Pipeline completed successfully!\n",
      "Repository URL: https://github.com/Group8MLUL2/Group8_CT1\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(\"\\n=== Final Results ===\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"CV R2: {metrics['cv_r2_mean']:.4f}\")\n",
    "    print(f\"Test R2: {metrics['test_r2']:.4f}\")\n",
    "    print(f\"Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "\n",
    "print(\"\\nSaving best model to repository...\")\n",
    "# Save the best model directly\n",
    "github_manager.create_or_update_file(\n",
    "    \"models/best_model.pkl\",\n",
    "    best_model,  # Pass the model object directly\n",
    "    \"Update best model\"\n",
    ")\n",
    "# Ensure models directory exists\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Method 1: Joblib save (recommended for sklearn models)\n",
    "joblib.dump(best_model, 'models/best_model_joblib.pkl', compress=True)\n",
    "\n",
    "# Method 2: Pickle save (alternative method)\n",
    "with open('models/best_model_pickle.pkl', 'wb') as f:\n",
    "    pickle.dump(best_model, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# Also save model using joblib for backup\n",
    "# Method 1: Save to file\n",
    "joblib.dump(best_model, 'best_model_joblib.pkl')\n",
    "\n",
    "# Method 2: Save to a bytes buffer\n",
    "model_buffer = io.BytesIO()\n",
    "joblib.dump(best_model, model_buffer)\n",
    "model_bytes = model_buffer.getvalue()\n",
    "\n",
    "# When uploading to GitHub, use model_bytes\n",
    "github_manager.create_or_update_file(\n",
    "    \"models/best_model_joblib.pkl\",\n",
    "    model_bytes,\n",
    "    \"Update best model (joblib format)\"\n",
    ")\n",
    "\n",
    "print(\"\\nPipeline completed successfully!\")\n",
    "print(f\"Repository URL: {github_manager.repo.html_url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652367cd-6b5e-4c3d-96a1-02f1425ec197",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
